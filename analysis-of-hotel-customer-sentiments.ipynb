{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/analysis-of-hotel-customer-sentiments?scriptVersionId=129061321\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Analysis of Hotel Customer Sentiments\n\n- Sentiment analysis is part of the Natural Language Processing (NLP) techniques that consists in extracting emotions related to some raw texts. This is usually used on social media posts and customer reviews in order to automatically understand if some users are positive or negative and why. The goal of this study is to show how sentiment analysis can be performed using python. Here are some of the main libraries we will use:\n\n  - NLTK: the most famous python module for NLP techniques\n  - Gensim: a topic-modeling and vector space modeling toolkit\n  - Scikit-learn: the most used python machine-learning library\n\n- We will use here some hotel review data. Each observation consists of one customer review for one hotel. Each customer review is composed of textual feedback of the customer's experience at the hotel and an overall rating. \n\n- For each textual review, we want to predict if it corresponds to a good review (the customer is happy) or to a bad one (the customer is not satisfied). The review's overall ratings can range from 2.5/10 to 10/10. \n  - In order to simplify the problem we will split those into two categories:\n\n    - bad reviews have overall ratings < 5\n    - good reviews have overall ratings >= 5\n\n- The challenge here is to be able to predict this information using only the raw textual data from the review.\n\n### Why do your guest write reviews?\n- Are they autobiographers? Or are they interested in sharing experiences as part of the new “social contract,” as consumers leverage reviews to make booking decisions?\n\n### What types of experiences are they sharing?\n- This dependent of the property of course, but reviews could be about many different parts of your hotel, including the amenities, location, room type, service, and others. Are you seeing any themes? Are these themes different than your competitors? If yes, these themes could be powerful advantages or disadvantages that your property has by comparison.\n\n### How does the review help us?\n- Ideally, a review is user-generated content that promotes your property. It can also have additional benefits. Perhaps, for example, a review writer highlights an amenity for children that is not advertised, like. a complimentary scoop of ice cream with dinner. A charming experience shared by a reviewer online may go long way for a family planning a trip. While the ice cream would probably not be the primary driver for booking decisions, the review promotes the hotel as a family-friendly environment. The happiness and joy shared by the reviewer could be the tie-breaker between your hotel and a competitor.\n\n### What about your team?\n- Perhaps you read a review about team member who delivered exceptional service and you share that comment with the team. How will your team react to that kind of positive motivation? Does this help raise morale and drive performance?\n\n### Do negative reviews hurt us?\n- Does a bad review always hurt the property? The answer is, it doesn’t have to. For example, I was recently shopping for online for my oldest child’s first bicycle. One review recounted a story of bicycle that arrived without pedals, which was not discovered until Christmas Eve. The manufacturer responded to the review in a thoughtful and genuine manner, apologizing for the mistake and highlighting the ways the company would avoid making the mistake in the future. Because of that response, I am considering purchasing this bicycle.","metadata":{}},{"cell_type":"markdown","source":"# Import library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport string\n\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer\n#  https://www.nltk.org/data.html\n\n# I added the necessary libraries during coding.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-10T16:53:59.157591Z","iopub.execute_input":"2023-05-10T16:53:59.158267Z","iopub.status.idle":"2023-05-10T16:54:01.070236Z","shell.execute_reply.started":"2023-05-10T16:53:59.158137Z","shell.execute_reply":"2023-05-10T16:54:01.068743Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"reviews_df = pd.read_csv(\"/kaggle/input/515k-hotel-reviews-data-in-europe/Hotel_Reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:01.072862Z","iopub.execute_input":"2023-05-10T16:54:01.073288Z","iopub.status.idle":"2023-05-10T16:54:07.818515Z","shell.execute_reply.started":"2023-05-10T16:54:01.073251Z","shell.execute_reply":"2023-05-10T16:54:07.817309Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:07.820027Z","iopub.execute_input":"2023-05-10T16:54:07.820625Z","iopub.status.idle":"2023-05-10T16:54:07.854811Z","shell.execute_reply.started":"2023-05-10T16:54:07.82059Z","shell.execute_reply":"2023-05-10T16:54:07.853504Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                       Hotel_Address  \\\n0   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n1   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n2   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n3   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n4   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n\n   Additional_Number_of_Scoring Review_Date  Average_Score   Hotel_Name  \\\n0                           194    8/3/2017            7.7  Hotel Arena   \n1                           194    8/3/2017            7.7  Hotel Arena   \n2                           194   7/31/2017            7.7  Hotel Arena   \n3                           194   7/31/2017            7.7  Hotel Arena   \n4                           194   7/24/2017            7.7  Hotel Arena   \n\n  Reviewer_Nationality                                    Negative_Review  \\\n0              Russia    I am so angry that i made this post available...   \n1             Ireland                                         No Negative   \n2           Australia    Rooms are nice but for elderly a bit difficul...   \n3      United Kingdom    My room was dirty and I was afraid to walk ba...   \n4         New Zealand    You When I booked with your company on line y...   \n\n   Review_Total_Negative_Word_Counts  Total_Number_of_Reviews  \\\n0                                397                     1403   \n1                                  0                     1403   \n2                                 42                     1403   \n3                                210                     1403   \n4                                140                     1403   \n\n                                     Positive_Review  \\\n0   Only the park outside of the hotel was beauti...   \n1   No real complaints the hotel was great great ...   \n2   Location was good and staff were ok It is cut...   \n3   Great location in nice surroundings the bar a...   \n4    Amazing location and building Romantic setting    \n\n   Review_Total_Positive_Word_Counts  \\\n0                                 11   \n1                                105   \n2                                 21   \n3                                 26   \n4                                  8   \n\n   Total_Number_of_Reviews_Reviewer_Has_Given  Reviewer_Score  \\\n0                                           7             2.9   \n1                                           7             7.5   \n2                                           9             7.1   \n3                                           1             3.8   \n4                                           3             6.7   \n\n                                                Tags days_since_review  \\\n0  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n1  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n2  [' Leisure trip ', ' Family with young childre...            3 days   \n3  [' Leisure trip ', ' Solo traveler ', ' Duplex...            3 days   \n4  [' Leisure trip ', ' Couple ', ' Suite ', ' St...           10 days   \n\n         lat       lng  \n0  52.360576  4.915968  \n1  52.360576  4.915968  \n2  52.360576  4.915968  \n3  52.360576  4.915968  \n4  52.360576  4.915968  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Hotel_Address</th>\n      <th>Additional_Number_of_Scoring</th>\n      <th>Review_Date</th>\n      <th>Average_Score</th>\n      <th>Hotel_Name</th>\n      <th>Reviewer_Nationality</th>\n      <th>Negative_Review</th>\n      <th>Review_Total_Negative_Word_Counts</th>\n      <th>Total_Number_of_Reviews</th>\n      <th>Positive_Review</th>\n      <th>Review_Total_Positive_Word_Counts</th>\n      <th>Total_Number_of_Reviews_Reviewer_Has_Given</th>\n      <th>Reviewer_Score</th>\n      <th>Tags</th>\n      <th>days_since_review</th>\n      <th>lat</th>\n      <th>lng</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n      <td>194</td>\n      <td>8/3/2017</td>\n      <td>7.7</td>\n      <td>Hotel Arena</td>\n      <td>Russia</td>\n      <td>I am so angry that i made this post available...</td>\n      <td>397</td>\n      <td>1403</td>\n      <td>Only the park outside of the hotel was beauti...</td>\n      <td>11</td>\n      <td>7</td>\n      <td>2.9</td>\n      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n      <td>0 days</td>\n      <td>52.360576</td>\n      <td>4.915968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n      <td>194</td>\n      <td>8/3/2017</td>\n      <td>7.7</td>\n      <td>Hotel Arena</td>\n      <td>Ireland</td>\n      <td>No Negative</td>\n      <td>0</td>\n      <td>1403</td>\n      <td>No real complaints the hotel was great great ...</td>\n      <td>105</td>\n      <td>7</td>\n      <td>7.5</td>\n      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n      <td>0 days</td>\n      <td>52.360576</td>\n      <td>4.915968</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n      <td>194</td>\n      <td>7/31/2017</td>\n      <td>7.7</td>\n      <td>Hotel Arena</td>\n      <td>Australia</td>\n      <td>Rooms are nice but for elderly a bit difficul...</td>\n      <td>42</td>\n      <td>1403</td>\n      <td>Location was good and staff were ok It is cut...</td>\n      <td>21</td>\n      <td>9</td>\n      <td>7.1</td>\n      <td>[' Leisure trip ', ' Family with young childre...</td>\n      <td>3 days</td>\n      <td>52.360576</td>\n      <td>4.915968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n      <td>194</td>\n      <td>7/31/2017</td>\n      <td>7.7</td>\n      <td>Hotel Arena</td>\n      <td>United Kingdom</td>\n      <td>My room was dirty and I was afraid to walk ba...</td>\n      <td>210</td>\n      <td>1403</td>\n      <td>Great location in nice surroundings the bar a...</td>\n      <td>26</td>\n      <td>1</td>\n      <td>3.8</td>\n      <td>[' Leisure trip ', ' Solo traveler ', ' Duplex...</td>\n      <td>3 days</td>\n      <td>52.360576</td>\n      <td>4.915968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n      <td>194</td>\n      <td>7/24/2017</td>\n      <td>7.7</td>\n      <td>Hotel Arena</td>\n      <td>New Zealand</td>\n      <td>You When I booked with your company on line y...</td>\n      <td>140</td>\n      <td>1403</td>\n      <td>Amazing location and building Romantic setting</td>\n      <td>8</td>\n      <td>3</td>\n      <td>6.7</td>\n      <td>[' Leisure trip ', ' Couple ', ' Suite ', ' St...</td>\n      <td>10 days</td>\n      <td>52.360576</td>\n      <td>4.915968</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Append the positive and negative text reviews","metadata":{}},{"cell_type":"code","source":"reviews_df[\"review\"] = reviews_df[\"Negative_Review\"] + reviews_df[\"Positive_Review\"]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:07.857811Z","iopub.execute_input":"2023-05-10T16:54:07.858219Z","iopub.status.idle":"2023-05-10T16:54:08.051947Z","shell.execute_reply.started":"2023-05-10T16:54:07.858182Z","shell.execute_reply":"2023-05-10T16:54:08.05074Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Create the label","metadata":{}},{"cell_type":"code","source":"reviews_df[\"is_bad_review\"] = reviews_df[\"Reviewer_Score\"].apply(lambda x: 1 if x < 5 else 0)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.053417Z","iopub.execute_input":"2023-05-10T16:54:08.053822Z","iopub.status.idle":"2023-05-10T16:54:08.488163Z","shell.execute_reply.started":"2023-05-10T16:54:08.053787Z","shell.execute_reply":"2023-05-10T16:54:08.48682Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Select only relevant columns","metadata":{}},{"cell_type":"code","source":"reviews_df = reviews_df[[\"review\", \"is_bad_review\"]]\nreviews_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.489724Z","iopub.execute_input":"2023-05-10T16:54:08.490127Z","iopub.status.idle":"2023-05-10T16:54:08.787889Z","shell.execute_reply.started":"2023-05-10T16:54:08.490091Z","shell.execute_reply":"2023-05-10T16:54:08.7866Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                              review  is_bad_review\n0   I am so angry that i made this post available...              1\n1  No Negative No real complaints the hotel was g...              0\n2   Rooms are nice but for elderly a bit difficul...              0\n3   My room was dirty and I was afraid to walk ba...              1\n4   You When I booked with your company on line y...              0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>is_bad_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I am so angry that i made this post available...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>No Negative No real complaints the hotel was g...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rooms are nice but for elderly a bit difficul...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>My room was dirty and I was afraid to walk ba...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You When I booked with your company on line y...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Sample data\n- Reviews data is sampled in order to speed up computations.","metadata":{}},{"cell_type":"code","source":"reviews_df = reviews_df.sample(frac = 0.1, replace = False, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.789783Z","iopub.execute_input":"2023-05-10T16:54:08.79062Z","iopub.status.idle":"2023-05-10T16:54:08.828004Z","shell.execute_reply.started":"2023-05-10T16:54:08.790569Z","shell.execute_reply":"2023-05-10T16:54:08.82664Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Clean data","metadata":{}},{"cell_type":"markdown","source":"### Remove 'No Negative' or 'No Positive' from text\n- If the user doesn't leave any negative feedback comment, this will appear as \"No Negative\" in our data.\n- This is the same for the positive comments with the default value \"No Positive\".\n- We have to remove those parts from our texts.","metadata":{}},{"cell_type":"code","source":"reviews_df[\"review\"] = reviews_df[\"review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.829813Z","iopub.execute_input":"2023-05-10T16:54:08.830221Z","iopub.status.idle":"2023-05-10T16:54:08.92262Z","shell.execute_reply.started":"2023-05-10T16:54:08.830182Z","shell.execute_reply":"2023-05-10T16:54:08.921357Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### NLTK is a leading platform for building Python programs to work with human language data.\n- It provides easy-to-use interfaces to over 50 corpora and lexical resources such as:\n    - WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.","metadata":{}},{"cell_type":"markdown","source":"### Return the wordnet object value corresponding to the POS tag","metadata":{}},{"cell_type":"code","source":"def get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.924105Z","iopub.execute_input":"2023-05-10T16:54:08.927631Z","iopub.status.idle":"2023-05-10T16:54:08.937495Z","shell.execute_reply.started":"2023-05-10T16:54:08.927582Z","shell.execute_reply":"2023-05-10T16:54:08.936008Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Clean text","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    # lower text\n    text = text.lower()\n    \n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    \n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    \n    # remove stop words\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    \n    # remove empty tokens\n    text = [t for t in text if len(t) > 0]\n    \n    # pos tag text\n    pos_tags = pos_tag(text)\n    \n    # lemmatize text\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    \n    # remove words with only one letter\n    text = [t for t in text if len(t) > 1]\n    \n    # join all\n    text = \" \".join(text)\n    return(text)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.939383Z","iopub.execute_input":"2023-05-10T16:54:08.939898Z","iopub.status.idle":"2023-05-10T16:54:08.954483Z","shell.execute_reply.started":"2023-05-10T16:54:08.939848Z","shell.execute_reply":"2023-05-10T16:54:08.953011Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.960184Z","iopub.execute_input":"2023-05-10T16:54:08.960615Z","iopub.status.idle":"2023-05-10T16:54:08.976906Z","shell.execute_reply.started":"2023-05-10T16:54:08.960576Z","shell.execute_reply":"2023-05-10T16:54:08.975607Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                   review  is_bad_review\n488440   Would have appreciated a shop in the hotel th...              0\n274649        No tissue paper box was present at the room              0\n374688                Pillows  Nice welcoming and service              0\n404352   Everything including the nice upgrade The Hot...              0\n451596                    Lovely hotel v welcoming staff               0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>is_bad_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>488440</th>\n      <td>Would have appreciated a shop in the hotel th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>274649</th>\n      <td>No tissue paper box was present at the room</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>374688</th>\n      <td>Pillows  Nice welcoming and service</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>404352</th>\n      <td>Everything including the nice upgrade The Hot...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>451596</th>\n      <td>Lovely hotel v welcoming staff</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# If you’re unsure of which datasets/models you’ll need, you can install the “popular” subset of NLTK data, on the command line type:\n#      python -m nltk.downloader popular\n\nimport nltk\nnltk.download('popular')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:08.978594Z","iopub.execute_input":"2023-05-10T16:54:08.980483Z","iopub.status.idle":"2023-05-10T16:54:10.419818Z","shell.execute_reply.started":"2023-05-10T16:54:08.98043Z","shell.execute_reply":"2023-05-10T16:54:10.418527Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"### Clean text data","metadata":{}},{"cell_type":"code","source":"reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T16:54:10.421643Z","iopub.execute_input":"2023-05-10T16:54:10.422412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To clean textual data, we call our custom 'clean_text' function that performs several transformations:\n- Lower the text\n- Tokenize the text (split the text into words) and remove the punctuation\n- Remove useless words that contain numbers\n- Remove useless stop words like 'the', 'a',' this' etc.\n- Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb, etc. using the WordNet lexical database\n- Lemmatize the text: transform every word into its root form (e.g. rooms -> room, slept -> sleep)","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"# add sentiment anaylsis columns\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Resource vader_lexicon not found.\n# Please use the NLTK Downloader to obtain the resource:\n# import nltk\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\nreviews_df[\"sentiments\"] = reviews_df[\"review\"].apply(lambda x: sid.polarity_scores(x))\nreviews_df = pd.concat([reviews_df.drop(['sentiments'], axis=1), reviews_df['sentiments'].apply(pd.Series)], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We first start by adding sentiment analysis features because we can guess that customers' reviews are highly linked to how they felt about their stay at the hotel. We use Vader, which is a part of the NLTK module designed for sentiment analysis. Vader uses a lexicon of words to find which ones are positives or negatives. It also takes into account the context of the sentences to determine the sentiment scores.\n\n- For each text, Vader returns 4 values:\n\n    - a neutrality score\n    - a positivity score\n    - a negativity score\n    - an overall score that summarizes the previous scores\n\n- We will integrate those 4 values as features in our dataset.","metadata":{}},{"cell_type":"code","source":"# add number of characters column\nreviews_df[\"nb_chars\"] = reviews_df[\"review\"].apply(lambda x: len(x))\n\n# add number of words column\nreviews_df[\"nb_words\"] = reviews_df[\"review\"].apply(lambda x: len(x.split(\" \")))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Next, we add some simple metrics for every text:\n    - number of characters in the text\n    - number of words in the text","metadata":{}},{"cell_type":"code","source":"# create doc2vec vector columns\n# gensim is a Python framework for fast Vector Space Modelling\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_df[\"review_clean\"].apply(lambda x: x.split(\" \")))]\n\n# train a Doc2Vec model with our text data\nmodel = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n\n# transform each document into a vector data\ndoc2vec_df = reviews_df[\"review_clean\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\ndoc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\nreviews_df = pd.concat([reviews_df, doc2vec_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The next step consists in extracting vector representations for every review.\n\n- The module Gensim creates a numerical vector representation of every word in the corpus by using the contexts in which they appear (Word2Vec). This is performed using shallow neural networks. What's interesting is that similar words will have similar representation vectors.\n\n- Each text can also be transformed into numerical vectors using the word vectors (Doc2Vec). Same texts will also have similar representations and that is why we can use those vectors as training features.\n\n- We first have to train a Doc2Vec model by feeding in our text data. By applying this model to our reviews, we can get those representation vectors.","metadata":{}},{"cell_type":"markdown","source":"### TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents (also known as a corpus).","metadata":{}},{"cell_type":"code","source":"# add tf-idfs columns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(min_df = 10)\ntfidf_result = tfidf.fit_transform(reviews_df[\"review_clean\"]).toarray()\ntfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names_out())\ntfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\ntfidf_df.index = reviews_df.index\nreviews_df = pd.concat([reviews_df, tfidf_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Finally, we add the TF-IDF (Term Frequency - Inverse Document Frequency) values for every word and every document.\n\n- But why not simply count how many times each word appears in every document? The problem with this method is that it doesn't take into account the relative importance of words in the texts. A word that appears in almost every text would not likely bring useful information for analysis. On the contrary, rare words may have a lot more meanings.\n\n- The TF-IDF metric solves this problem:\n    - TF computes the classic number of times the word appears in the text\n    - IDF computes the relative importance of this word which depends on how many texts the word can be found\n\n\n- We add TF-IDF columns for every word that appears in at least 10 different texts to filter some of them and reduce the size of the final output.","metadata":{}},{"cell_type":"code","source":"reviews_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis\n### In order to have a better understanding of our data, let's explore it a little:","metadata":{}},{"cell_type":"code","source":"# show is_bad_review distribution\nreviews_df[\"is_bad_review\"].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Our dataset is highly imbalanced because less than 5% of our reviews are considered negative ones. This information will be very useful for the modeling part.","metadata":{}},{"cell_type":"markdown","source":"### Now let's print some word clouds to have a glimpse at what kind of words appear in our reviews:","metadata":{}},{"cell_type":"code","source":"# wordcloud function\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 42\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n    \n# print wordcloud\nshow_wordcloud(reviews_df[\"review\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Most of the words are indeed related to the hotels: room, staff, breakfast, etc.\n- Some words are more related to the customer experience with the hotel stay: perfect, loved, expensive, dislike, etc.","metadata":{}},{"cell_type":"markdown","source":"### Highest positive sentiment reviews (with more than 5 words)","metadata":{}},{"cell_type":"code","source":"reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"pos\", ascending = False)[[\"review\", \"pos\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The most positive reviews indeed correspond to some good feedbacks.","metadata":{}},{"cell_type":"markdown","source":"### Lowest negative sentiment reviews (with more than 5 words)","metadata":{}},{"cell_type":"code","source":"reviews_df[reviews_df[\"nb_words\"] >= 5].sort_values(\"neg\", ascending = False)[[\"review\", \"neg\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Some errors can be found among the most negative reviews: Vader sometimes interprets 'no' or 'nothing' as negative words whereas they are sometimes used to say that there were no problems with the hotel.\n- Fortunately, most of the reviews are indeed bad ones.","metadata":{}},{"cell_type":"markdown","source":"### Plot sentiment distribution for positive and negative reviews","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nfor x in [0, 1]:\n    subset = reviews_df[reviews_df['is_bad_review'] == x]\n    \n    # Draw the density plot\n    if x == 0:\n        label = \"Good reviews\"\n    else:\n        label = \"Bad reviews\"\n    sns.distplot(subset['compound'], hist = False, label = label)\n    #sns.displot(subset['compound'], label = label, kind=\"kde\")\n    #sns.histplot(subset['compound'], label = label, kde=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The above graph shows the distribution of the reviews sentiments among good reviews and bad ones. We can see that good reviews are for most of them considered very positive by Vader. On the contrary, bad reviews tend to have lower compound sentiment scores.\n\n- This shows us that previously computed sentiment features will be very important in our modeling part.","metadata":{}},{"cell_type":"markdown","source":"# Modeling Reviewer Score","metadata":{}},{"cell_type":"code","source":"# feature selection\nlabel = \"is_bad_review\"\nignore_cols = [label, \"review\", \"review_clean\"]\nfeatures = [c for c in reviews_df.columns if c not in ignore_cols]\n\n# split the data into train and test\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(reviews_df[features], reviews_df[label], test_size = 0.20, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We first choose which features we want to use to train our model. Then we split our data into two parts:\n\n    - one to train our model\n    - one to assess its performances\n    \n\n- We will next use a Random Forest (RF) classifier for our predictions.","metadata":{}},{"cell_type":"code","source":"# train a random forest classifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 42)\nrf.fit(X_train, y_train)\n\n# show feature importance\nfeature_importances_df = pd.DataFrame({\"feature\": features, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending = False)\nfeature_importances_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The most important features are indeed the ones that come from the previous sentiment analysis.\n- The vector representations of the texts also have a lot of importance in our training.\n- Some words appear to have fairly good importance as well.","metadata":{}},{"cell_type":"markdown","source":"# ROC curve","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\n\ny_pred = [x[1] for x in rf.predict_proba(X_test)]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label = 1)\n\nroc_auc = auc(fpr, tpr)\n\nplt.figure(1, figsize = (15, 10))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The ROC (Receiver Operating Characteristic) curve is usually a good graph to summarize the quality of our classifier. The higher the curve is above the diagonal baseline, the better the predictions. Although the AUC ROC (Area Under the Curve ROC) is very good, we should not use here the ROC curve to assess the quality of our model.\n\n- Why? First let us remind the False Positive Rate formula, which corresponds to the x-axis of the ROC curve: FPR (False Positive Rate) = # False Positives / # Negatives.\n\n- Here the # Negatives corresponds to our number of good reviews which is very high because our dataset is imbalanced. This means that even with some False Positives, our FPR will tend to stay very low. Our model will be able to make a lot of false positive predictions and still have a low false positive rate while increasing the true positive rate and therefore artificially increasing the AUC ROC metric.","metadata":{}},{"cell_type":"markdown","source":"# PR(Precision-Recall) curve","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, precision_recall_curve\n#from sklearn.utils.fixes import signature\n!pip install funcsigs\nfrom funcsigs import signature\n\naverage_precision = average_precision_score(y_test, y_pred)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\n\nplt.figure(1, figsize = (15, 10))\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- A better metric in this imbalanced situation is the AUC PR (Area Under the Curve Precision Recall), also called AP (Average Precision).\n\n- We can see that the precision decreases when we increase the recall. This shows us that we have to choose a prediction threshold adapted to our needs. If our goal is to have a high recall, we should set a low prediction threshold that will allow us to detect most of the observations of the positive class, but with low precision. On the contrary, if we want to be really confident about our predictions but don't mind about not finding all the positive observations, we should set a high threshold that will get us a high precision and a low recall.\n\n- In order to know if our model performs better than another classifier, we can simply use the AP metric. To assess the quality of our model, we can compare it to a simple decision baseline. Let's take a random classifier as a baseline here that would predict half of the time 1 and half of the time 0 for the label.\n\n- Such a classifier would have a precision of 4.3%, which corresponds to the proportion of positive observations. For every recall value, the precision would stay the same, and this would lead us to an AP of 0.043. The AP of our model is approximately 0.38, which is more than 8 times higher than the AP of the random method. This means that our model has good predictive power.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n- It is completely possible to use only raw text as input for making predictions. The most important thing is to be able to extract the relevant features from this raw source of data. This kind of data can often come as a good complementary source in data science projects in order to extract more learning features and increase the predictive power of the models.","metadata":{}}]}